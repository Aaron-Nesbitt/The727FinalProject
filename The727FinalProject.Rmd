---
title: "The727FinalProject"
author: "Aaron Nesbitt and Jessica Valencia"
date: '2022-12-01'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)
library(rtweet)
library(ggplot2)
library(plyr)
library(tidyverse)
library(tidytext)
library(ggmap)
library(rJava)
library(qdap)
```

We search we original tweets, allowing for up to 18 thousand. We ran the function at 5 pm on Tuesday November 29th.

```{r}
#musktweetsfinal <- read_csv("C:\\Users\\valen\\Downloads\\musktweetsfinal.csv")
#muskTweets <- read_csv("C:\\Users\\aanes\\Downloads\\muskTweets.csv")
```

```{r eval=FALSE, include=FALSE}
#musk <- search_tweets(
#  q = "musk Twitter, lang:en",
#  n = 18000,
#  include_rts = FALSE
)
```

#Overall Analsysis of entire tweet corpus

```{r}
frequent_terms = freq_terms(musktweetsfinal["text"], 30)
plot(frequent_terms)
```

```{r}
musktweetsfinal$bagAllTweets = musktweetsfinal$text %>% iconv("latin1", "ASCII", sub="") %>% scrubber() %sw% qdapDictionaries::Top200Words
```

```{r}
frequent_terms2 = freq_terms(musktweetsfinal["bagAllTweets"], 30)
plot(frequent_terms2)
```
```{r}
frequent_terms3 = freq_terms(musktweetsfinal["bagAllTweets"], 100)
```

```{r}
freq_terms3_less <- frequent_terms3[-c(1,2,3,4,5,6,8),]
```

```{r}
install.packages("wordcloud")
install.packages("RColorBrewer")

library(wordcloud)
library(RColorBrewer)
```

```{r}
set.seed(4356)
wordcloud(words = freq_terms3_less$WORD, freq = freq_terms3_less$FREQ, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
```

```{r, cache=TRUE}
sentimentsAll = analyzeSentiment(iconv(as.character(musktweetsfinal$bagAllTweets), to='UTF-8'))
head(sentiments)
```

```{r}
install.packages("syuzhet")
library(syuzhet)
```

```{r}
emotionsALL<-get_nrc_sentiment(musktweetsfinal$bagAllTweets)

```

```{r}
emotionsALLt<-data.frame(t(emotionsALL))
#The function rowSums computes column sums across rows for each level of a grouping variable.
plotemot <- data.frame(rowSums(emotionsALLt[1:17412]))
#Transformation and cleaning
names(plotemot)[1] <- "count"
plotemot <- cbind("sentiment" = rownames(plotemot), plotemot)
rownames(plotemot) <- NULL
plotemot2<-plotemot[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=plotemot2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")

```



```{r}
head(musktweetsfinal$text, n=10)
```

```{r}
colnames(musktweetsfinal)
```

```{r}
library(quanteda)

opinionWords = c('Apple', 'apple','@apple', '@Apple', '#Apple', '#apple', 'Apple\'s', 'apple\'s','-Apple','Apples','@AppleSupport','@AppleTVPlus','#MuskTwitterAppleMeltdown','@Apple!','#AppleMusicLive')

muskTweets = data.frame()
doNotNeed = data.frame()

for(i in 1:nrow(musktweetsfinal)){
  # break up tweet into individual words
  tweet = musktweetsfinal$text[i]
  tweet = tolower(tweet)
  words = tokens(tweet)
  # if it contains an irrelevant word, put it in the irrelevant data frame
  if(length(intersect(words[['text1']], opinionWords))>0){
    doNotNeed = rbind(doNotNeed, musktweetsfinal[i,])
  }
  else{
    muskTweets = rbind(muskTweets, musktweetsfinal[i,])
  }
}
```




This is a sentiment analysis of the tweets. 
```{r, eval=FALSE}
install.packages("SentimentAnalysis")
library(SentimentAnalysis)
sentiments = analyzeSentiment(iconv(as.character(muskTweets$text), to='UTF-8'))
head(sentiments)
```

```{r}
#PULL IN DATA FROM PREVIOUS CHUNK USING CSV FILE 
#library(readr)
#sentiments <- read_csv("C:/Users/aanes/Downloads/sentiments.csv")
#View(sentiments)
```

```{r}
#library(vader)
# = vader_df(muskTweets$text)
#sentiments$Vader = vader_scores$compound
#ggplot(sentiments, aes(x=WordCount, y=Vader)) + geom_point(alpha=.1) + geom_smooth()
```




```{r}
ggplot(sentiments, aes(x=WordCount, y=SentimentQDAP)) + geom_point(alpha=.1) + geom_smooth()
```

```{r}
library(vader)
test <- muskTweets[1:1000,]
vader_scores = vader_df(test$text)
```
```{r}
test2 <- muskTweets[1001:3000,]
vader_scores2 = vader_df(test2$text)
```



```{r}
test3 <- muskTweets[3001:5000,]
vader_scores3 = vader_df(test3$text)
```
```{r}
test4 <- muskTweets[5001:7000,]
vader_scores4 = vader_df(test4$text)
```

```{r}
test5 <- muskTweets[7001:9000,]
vader_scores5 = vader_df(test5$text)
test6 <- muskTweets[9001:11000,]
vader_scores6 = vader_df(test6$text)
```

```{r}
test7 <- muskTweets[11001:13000,]
vader_scores7 = vader_df(test7$text)
test8 <- muskTweets[13001:14193,]
vader_scores8 = vader_df(test8$text)
```


```{r}
allvader <- rbind(vader_scores, vader_scores2, vader_scores3,vader_scores4,vader_scores5,vader_scores6,vader_scores7,vader_scores8 )
```



```{r}
sentiments$Vader = allvader$compound
ggplot(sentiments, aes(x=WordCount, y=Vader)) + geom_point(alpha=.1) + geom_smooth()
```


```{r}
write.csv(sentiments, "C:\\Users\\aanes\\Downloads\\sentiments.csv")
```

```{r}
count<- freq(muskTweets$source)

```


